# =========================================================
# GDP AUTOMATION PIPELINE (PRODUCTION)
# =========================================================

import requests
import pandas as pd
import itertools
from urllib.parse import quote
import logging
from datetime import datetime
import os
import sys
from google.cloud import bigquery
from google.oauth2 import service_account
import json

# ---------------------------------------------------------
# PATHS
# ---------------------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, "output")
LOG_DIR = os.path.join(BASE_DIR, "logs")

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ---------------------------------------------------------
# LOGGING
# ---------------------------------------------------------
log_file = os.path.join(LOG_DIR, "pipeline.log")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler(sys.stdout)
    ]
)

TIMEOUT = 30

# ---------------------------------------------------------
# BIGQUERY AUTH
# ---------------------------------------------------------
if "DATA_SERVICE_ACCOUNT_KEY" not in os.environ:
    raise EnvironmentError("‚ùå DATA_SERVICE_ACCOUNT_KEY secret –æ–ª–¥—Å–æ–Ω–≥“Ø–π")

credentials_info = json.loads(os.environ["DATA_SERVICE_ACCOUNT_KEY"])

credentials = service_account.Credentials.from_service_account_info(
    credentials_info
)

bq_client = bigquery.Client(credentials=credentials)

# ---------------------------------------------------------
# FUNCTIONS
# ---------------------------------------------------------
def get_table_metadata(table_path):
    encoded_path = quote(table_path, safe="/")
    url = f"https://data.1212.mn/api/v1/mn/NSO/{encoded_path}"
    r = requests.get(url, timeout=TIMEOUT)
    r.raise_for_status()
    return r.json()


def get_nso_data(table_path, payload):
    encoded_path = quote(table_path, safe="/")
    url = f"https://data.1212.mn/api/v1/mn/NSO/{encoded_path}"
    r = requests.post(url, json=payload, timeout=TIMEOUT)
    r.raise_for_status()
    return r.json()


def jsonstat_to_dataframe(data):
    dimensions = data["dimension"]
    values = data["value"]
    dim_names = data["id"]

    dim_labels = {}
    dim_sizes = []

    for dim in dim_names:
        labels = dimensions[dim]["category"]["label"]
        dim_labels[dim] = list(labels.values())
        dim_sizes.append(len(labels))

    rows = []
    for idx, combo in enumerate(itertools.product(*[range(s) for s in dim_sizes])):
        row = {}
        for i, dim in enumerate(dim_names):
            row[dim] = dim_labels[dim][combo[i]]
        row["DTVAL_CO"] = values[idx]
        rows.append(row)

    return pd.DataFrame(rows)


def pivot_validate(df, mapping, label):
    if "–ë“Ø—Ä—ç–ª–¥—ç—Ö“Ø“Ø–Ω" not in df.columns:
        raise KeyError(f"{label}: '–ë“Ø—Ä—ç–ª–¥—ç—Ö“Ø“Ø–Ω' –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π")

    df["component"] = df["–ë“Ø—Ä—ç–ª–¥—ç—Ö“Ø“Ø–Ω"].replace(mapping)

    pv = (
        df.pivot_table(
            index="–û–ù",
            columns="component",
            values="DTVAL_CO",
            aggfunc="first"
        )
        .reset_index()
    )

    ordered_cols = ["–û–ù"] + list(mapping.values())
    pv = pv.reindex(columns=ordered_cols)
    pv = pv.fillna(0)

    if pv.empty:
        raise ValueError(f"{label} pivot —Ö–æ–æ—Å–æ–Ω –±–∞–π–Ω–∞")

    missing = set(ordered_cols) - set(pv.columns)
    if missing:
        raise ValueError(f"{label} pivot –±–∞–≥–∞–Ω–∞ –¥—É—Ç—É—É: {missing}")

    logging.info(f"üìä {label} pivot OK")
    return pv

# ---------------------------------------------------------
# MAIN PIPELINE
# ---------------------------------------------------------
def main():
    logging.info("üöÄ GDP pipeline —ç—Ö—ç–ª–ª—ç—ç")

    table_path = "Economy, environment/National Accounts/DT_NSO_0500_022V1.px"
    metadata = get_table_metadata(table_path)

    def build_query(stat_code):
        query = {"query": [], "response": {"format": "json-stat2"}}
        for var in metadata["variables"]:
            if var["text"] == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫ “Ø–∑“Ø“Ø–ª—ç–ª—Ç":
                query["query"].append({
                    "code": var["code"],
                    "selection": {"filter": "item", "values": [stat_code]}
                })
            else:
                query["query"].append({
                    "code": var["code"],
                    "selection": {"filter": "item", "values": var["values"]}
                })
        return query

    # === MAPS ===
    ngdp_map = {
        "–î–ù–ë": "ngdp",
        "–•”©–¥”©”© –∞–∂ –∞—Ö—É–π, –æ–π–Ω –∞–∂ –∞—Ö—É–π, –∑–∞–≥–∞—Å –±–∞—Ä–∏–ª—Ç, –∞–Ω –∞–≥–Ω—É—É—Ä": "ngdp_agri",
        "–£—É–ª —É—É—Ä—Ö–∞–π, –æ–ª–±–æ—Ä–ª–æ–ª—Ç": "ngdp_mine",
        "–ë–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö “Ø–π–ª–¥–≤—ç—Ä–ª—ç–ª": "ngdp_manu",
        "–¶–∞—Ö–∏–ª–≥–∞–∞–Ω, —Ö–∏–π, —É—É—Ä, –∞–≥–∞–∞—Ä–∂—É—É–ª–∞–ª—Ç": "ngdp_elec",
        "–ë–∞—Ä–∏–ª–≥–∞": "ngdp_cons",
        "–ë”©”©–Ω–∏–π –±–æ–ª–æ–Ω –∂–∏–∂–∏–≥–ª—ç–Ω —Ö—É–¥–∞–ª–¥–∞–∞, –º–∞—à–∏–Ω, –º–æ—Ç–æ—Ü–∏–∫–ª–∏–π–Ω –∑–∞—Å–≤–∞—Ä, “Ø–π–ª—á–∏–ª–≥—ç—ç": "ngdp_trad",
        "–¢—ç—ç–≤—ç—Ä –±–∞ –∞–≥—É—É–ª–∞—Ö—ã–Ω “Ø–π–ª –∞–∂–∏–ª–ª–∞–≥–∞–∞": "ngdp_tran",
        "–ú—ç–¥—ç—ç–ª—ç–ª, —Ö–æ–ª–±–æ–æ": "ngdp_info",
        "“Æ–π–ª—á–∏–ª–≥—ç—ç–Ω–∏–π –±—É—Å–∞–¥ “Ø–π–ª –∞–∂–∏–ª–ª–∞–≥–∞–∞": "ngdp_oser",
        "–ë“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π —Ü—ç–≤—ç—Ä —Ç–∞—Ç–≤–∞—Ä": "ngdp_taxe"
    }
    
    df_ngdp = jsonstat_to_dataframe(get_nso_data(table_path, build_query("0")))
    pv_ngdp = pivot_validate(df_ngdp, ngdp_map, "NGDP")
    # ===================== RGDP by 2005 =====================
    rgdp_2005_map = {k: f"rgdp_2005{v[4:]}" for k, v in ngdp_map.items()}
    
    df_rgdp_2005 = jsonstat_to_dataframe(get_nso_data(table_path, build_query("1")))
    pv_rgdp_2005 = pivot_validate(df_rgdp_2005, rgdp_2005_map, "RGDP 2005")
    
    # ===================== RGDP by 2010 =====================
    rgdp_2010_map = {k: f"rgdp_2010{v[4:]}" for k, v in ngdp_map.items()}
    
    df_rgdp_2010 = jsonstat_to_dataframe(get_nso_data(table_path, build_query("2")))
    pv_rgdp_2010 = pivot_validate(df_rgdp_2010, rgdp_2010_map, "RGDP 2010")
    
    # ===================== RGDP by 2015 =====================
    rgdp_2015_map = {k: f"rgdp_2015{v[4:]}" for k, v in ngdp_map.items()}
    
    df_rgdp_2015 = jsonstat_to_dataframe(get_nso_data(table_path, build_query("3")))
    pv_rgdp_2015 = pivot_validate(df_rgdp_2015, rgdp_2015_map, "RGDP 2015")

    # ===================== GROWTH =====================
    growth_map = {k: f"growth{v[4:]}" for k, v in ngdp_map.items()}
    
    df_growth = jsonstat_to_dataframe(get_nso_data(table_path, build_query("6")))
    pv_growth = pivot_validate(df_growth, growth_map, "GDP Growth")
        # ===================== POPULATION =====================
    pop_table_path = "Population, household/1_Population, household/DT_NSO_0300_003V1.px"

    pop_payload = {
        "query": [
            {"code": "–•“Ø–π—Å", "selection": {"filter": "item", "values": ["0", "1", "2"]}},
            {"code": "–ù–∞—Å–Ω—ã –±“Ø–ª—ç–≥", "selection": {"filter": "item", "values": [str(i) for i in range(16)]}},
            {"code": "–û–Ω", "selection": {"filter": "item", "values": [str(i) for i in range(40)]}},
        ],
        "response": {"format": "json-stat2"}
    }

    df_population = jsonstat_to_dataframe(
        get_nso_data(pop_table_path, pop_payload)
    )

    pv_population = (
        df_population
        .pivot_table(
            index=["–•“Ø–π—Å", "–ù–∞—Å–Ω—ã –±“Ø–ª—ç–≥"],
            columns="–û–Ω",
            values="DTVAL_CO",
            aggfunc="sum"
        )
        .reset_index()
    )
    pop_long = pv_population.melt(
    id_vars=["–•“Ø–π—Å", "–ù–∞—Å–Ω—ã –±“Ø–ª—ç–≥"],
    var_name="year",
    value_name="value"
    )
    
    pop_long["indicator_code"] = "population"
    pop_long["source"] = "NSO 1212.mn"
    pop_long["loaded_at"] = pd.Timestamp.utcnow()
    pop_long["topic"] = "population"

    

    logging.info("üìä Population pivot OK")

    final_df = (
        pv_ngdp
        .merge(pv_rgdp_2005, on="–û–ù", how="outer")
        .merge(pv_rgdp_2010, on="–û–ù", how="outer")
        .merge(pv_rgdp_2015, on="–û–ù", how="outer")
        .merge(pv_growth, on="–û–ù", how="outer")
        )
    final_df = final_df.fillna(0)


    # ===================== EXPORT =====================
    if final_df.empty:
        raise ValueError("‚ùå final_df —Ö–æ–æ—Å–æ–Ω –±–∞–π–Ω–∞, —ç–∫—Å–ø–æ—Ä—Ç —Ö–∏–π—Ö –±–æ–ª–æ–º–∂–≥“Ø–π")
    
    # –ë–∞–≥–∞–Ω–∞ –¥–∞—Ä–∞–∞–ª–∞–ª (–û–ù —ç—Ö—ç–Ω–¥)
    cols = ["–û–ù"] + [c for c in final_df.columns if c != "–û–ù"]
    final_df = final_df[cols]
    
    output_file = os.path.join(
        OUTPUT_DIR,
        f"GDP_pipeline_{datetime.now().strftime('%Y%m%d')}.xlsx"
    )
    
    with pd.ExcelWriter(output_file, engine="xlsxwriter") as writer:
        final_df.to_excel(writer, sheet_name="GDP", index=False)
        pv_population.to_excel(writer, sheet_name="Population", index=False)

        # ===================== LOAD TO BIGQUERY (RAW, NO CHANGE) =====================
    table_id = "astute-azimuth-485909-p6.Automation_data.test_table"

    # Wide ‚Üí Long (—è–º–∞—Ä —á drop / filter —Ö–∏–π—Ö–≥“Ø–π)
    id_col = "–û–ù"
    value_cols = [c for c in final_df.columns if c != id_col]

    long_df = final_df.melt(
        id_vars=id_col,
        value_vars=value_cols,
        var_name="indicator_code",
        value_name="value"
    )

    long_df = long_df.rename(columns={"–û–ù": "year"})
    long_df["source"] = "NSO 1212.mn"
    long_df["loaded_at"] = pd.Timestamp.utcnow()
    long_df["topic"] = "gdp" #Sheet name option
    # ===================== POPULATION ‚Üí LONG =====================
    pop_long = pv_population.melt(
        id_vars=["–•“Ø–π—Å", "–ù–∞—Å–Ω—ã –±“Ø–ª—ç–≥"],
        var_name="year",
        value_name="value"
    )
    
    pop_long = pop_long.rename(columns={
        "–•“Ø–π—Å": "sex",
        "–ù–∞—Å–Ω—ã –±“Ø–ª—ç–≥": "age_group"
    })
    
    pop_long["topic"] = "population"
    pop_long["source"] = "NSO 1212.mn"
    pop_long["loaded_at"] = pd.Timestamp.utcnow()

    pop_long["topic"] = "population"

    # ===================== FINAL MERGE =====================
    final_long = pd.concat(
        [long_df, pop_long],
        ignore_index=True
    )
    
    job = bq_client.load_table_from_dataframe(
        final_long,
        table_id,
        job_config=bigquery.LoadJobConfig(
            write_disposition="WRITE_TRUNCATE"
        )
    )



    job.result()
    logging.info(f"‚òÅÔ∏è BigQuery-–¥ {len(final_long)} –º”©—Ä (GDP + Population) –±–∏—á–∏–≥–¥–ª—ç—ç")

    
    logging.info(f"‚úÖ Pipeline –∞–º–∂–∏–ª—Ç—Ç–∞–π –¥—É—É—Å–ª–∞–∞ ‚Üí {output_file}")

# ---------------------------------------------------------
# ENTRY POINT
# ---------------------------------------------------------
if __name__ == "__main__":
    main()
